{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================\n",
      "                                                            \n",
      "    üîç SISTEM DETEKSI ADUAN TRANSPORTASI DAN LALU LINTAS OTOMATIS                     \n",
      "    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                \n",
      "\n",
      "    üìä Powered by Machine Learning & NLP                     \n",
      "    üöÄ Enhanced CLI Interface                                 \n",
      "    ‚ö° Real-time Text Analysis                                \n",
      "                                                            \n",
      "================================================================\n",
      "\n",
      "‚úÖ Menyiapkan model & data Complete!\n",
      "\n",
      "üì¢ Sistem siap digunakan. Ketik 'exit' untuk keluar.\n",
      "==================================================================\n",
      "üïì Waktu Prediksi        : 2025-08-05 05:00:03\n",
      "üóíÔ∏è  Teks Asli            : Di daerah Ketintang Timur Surabaya telah terjadi kecelakaan yang melibatkan 2 pengendara motor\n",
      "üëâ Label                 : Aduan\n",
      "üî¢ Probabilitas          : 1.0000\n",
      "üßÆ Panjang Teks          : 94 karakter\n",
      "üìè Jumlah Token          : 0 kata\n",
      "‚è±Ô∏è  Durasi Prediksi      : 0.0001 detik\n",
      "------------------------------------------------------------------\n",
      "üëã Terima kasih. Sampai jumpa.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# ==== KONFIGURASI FILE ====\n",
    "BOOSTER_JSON_PATH = \"source/booster.json\"\n",
    "BOOSTER_CONFIG_PATH = \"source/booster_config.json\"\n",
    "VECT_PATH = \"model/vectorizer-fiks-1.pkl\"\n",
    "XGB_PATH = \"model/xgboost_model-fiks-1.pkl\"\n",
    "KEYW_PATH = \"model/aduan_keywords-1.npy\"\n",
    "SLANG_PATH = \"source/slang-kamus.txt\"\n",
    "THRESHOLD = 0.4\n",
    "\n",
    "# ==== FUNGSI UTIL ====\n",
    "def print_banner():\n",
    "    banner = \"\"\"\n",
    "================================================================\n",
    "                                                            \n",
    "    üîç SISTEM DETEKSI ADUAN TRANSPORTASI DAN LALU LINTAS OTOMATIS                     \n",
    "    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                \n",
    "\n",
    "    üìä Powered by Machine Learning & NLP                     \n",
    "    üöÄ Enhanced CLI Interface                                 \n",
    "    ‚ö° Real-time Text Analysis                                \n",
    "                                                            \n",
    "================================================================\n",
    "\"\"\"\n",
    "    print(banner)\n",
    "\n",
    "def print_separator(char=\"=\", length=66):\n",
    "    print(char * length)\n",
    "\n",
    "def print_loading(message=\"Loading\"):\n",
    "    for i in range(4):\n",
    "        sys.stdout.write(f\"\\r‚è≥ {message}{'.' * i}   \")\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(0.3)\n",
    "    sys.stdout.write(f\"\\r‚úÖ {message} Complete!\\n\")\n",
    "\n",
    "# ==== LOAD DATA ====\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_slang_dict():\n",
    "    slang = {}\n",
    "    try:\n",
    "        with open(SLANG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if \":\" in line:\n",
    "                    k, v = line.strip().split(\":\")\n",
    "                    slang[k] = v\n",
    "    except:\n",
    "        pass\n",
    "    return slang\n",
    "\n",
    "# ==== LOGIKA BOOSTER ====\n",
    "def is_booster_aduan(text, token_pairs, regex_patterns):\n",
    "    tokens = set(word_tokenize(text.lower()))\n",
    "    for pair in token_pairs:\n",
    "        if all(p in tokens for p in pair):\n",
    "            return True, f\"[TokenPairBooster: {pair}]\"\n",
    "    for rule in regex_patterns:\n",
    "        if re.search(fr\"{rule['head']}\\s+{rule['tail']}\", text.lower()):\n",
    "            return True, f\"[RegexBooster: {rule}]\"\n",
    "    return False, None\n",
    "\n",
    "def normalize_slang(text, slang_dict):\n",
    "    return \" \".join([slang_dict.get(w, w) for w in word_tokenize(text.lower())])\n",
    "\n",
    "def stem_and_check(word, stemmer, keywords):\n",
    "    stem = stemmer.stem(word)\n",
    "    return int(stem in keywords), stem\n",
    "\n",
    "def preprocess(text, slang_dict, stemmer, stop_words, keywords):\n",
    "    text = normalize_slang(text, slang_dict)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    tokens = [w for w in word_tokenize(text) if w not in stop_words]\n",
    "    clean = []\n",
    "    score = 0\n",
    "    for w in tokens:\n",
    "        c, s = stem_and_check(w, stemmer, keywords)\n",
    "        clean.append(s)\n",
    "        score += c\n",
    "    return \" \".join(clean), score, len(tokens)\n",
    "\n",
    "def is_keyword_aduan(text, keywords):\n",
    "    return int(bool(set(word_tokenize(text.lower())) & keywords))\n",
    "\n",
    "# ==== PREDIKSI UTAMA ====\n",
    "def predict_aduan(text, booster_aduan, booster_bukan_aduan, token_pairs, regex_patterns,\n",
    "                vectorizer, model, keywords, slang_dict, stemmer, stop_words):\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    start = time.time()\n",
    "\n",
    "    for pat in booster_aduan:\n",
    "        if re.search(pat, text_lower):\n",
    "            duration = time.time() - start\n",
    "            return 1, 1.0, 0, text, f\"[AduanBooster: {pat}]\", len(text), 0, duration\n",
    "\n",
    "    ok, note = is_booster_aduan(text, token_pairs, regex_patterns)\n",
    "    if ok:\n",
    "        duration = time.time() - start\n",
    "        return 1, 1.0, 0, text, note, len(text), 0, duration\n",
    "\n",
    "    cleaned, senti_score, token_count = preprocess(text, slang_dict, stemmer, stop_words, keywords)\n",
    "    Xv = vectorizer.transform([cleaned])\n",
    "    Xk = np.array([is_keyword_aduan(cleaned, keywords)]).reshape(-1, 1)\n",
    "    X = hstack([Xv, Xk])\n",
    "    prob = model.predict_proba(X)[0, 1]\n",
    "    label = int(prob >= THRESHOLD)\n",
    "\n",
    "    if label == 1:\n",
    "        duration = time.time() - start\n",
    "        return label, prob, senti_score, cleaned, \"ML-Prediksi\", len(text), token_count, duration\n",
    "\n",
    "    for pat in booster_bukan_aduan:\n",
    "        if re.search(pat, text_lower):\n",
    "            duration = time.time() - start\n",
    "            return 0, prob, senti_score, cleaned, f\"[BukanAduanBooster: {pat}]\", len(text), token_count, duration\n",
    "\n",
    "    duration = time.time() - start\n",
    "    return label, prob, senti_score, cleaned, \"ML-Prediksi\", len(text), token_count, duration\n",
    "\n",
    "# ==== MAIN PROGRAM ====\n",
    "if __name__ == \"__main__\":\n",
    "    print_banner()\n",
    "    print_loading(\"Menyiapkan model & data\")\n",
    "\n",
    "    booster_data = load_json(BOOSTER_JSON_PATH)\n",
    "    booster_aduan = booster_data[\"booster_aduan\"]\n",
    "    booster_bukan_aduan = booster_data[\"booster_bukan_aduan\"]\n",
    "\n",
    "    config_data = load_json(BOOSTER_CONFIG_PATH)\n",
    "    token_pairs = config_data[\"token_booster_pairs\"]\n",
    "    regex_patterns = config_data[\"regex_booster_patterns\"]\n",
    "\n",
    "    vectorizer = joblib.load(VECT_PATH)\n",
    "    model = joblib.load(XGB_PATH)\n",
    "    keywords = set(np.load(KEYW_PATH, allow_pickle=True))\n",
    "    slang_dict = load_slang_dict()\n",
    "    stemmer = StemmerFactory().create_stemmer()\n",
    "    stop_words = set(stopwords.words(\"indonesian\")) | set(stopwords.words(\"english\"))\n",
    "\n",
    "    print(\"\\nüì¢ Sistem siap digunakan. Ketik 'exit' untuk keluar.\")\n",
    "\n",
    "    while True:\n",
    "        text = input(\"\\nüìù Masukkan teks: \").strip()\n",
    "        if text.lower() == \"exit\":\n",
    "            print(\"üëã Terima kasih. Sampai jumpa.\")\n",
    "            break\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        label, prob, score, cleaned, info, char_len, token_len, durasi = predict_aduan(\n",
    "            text, booster_aduan, booster_bukan_aduan, token_pairs,\n",
    "            regex_patterns, vectorizer, model, keywords,\n",
    "            slang_dict, stemmer, stop_words\n",
    "        )\n",
    "\n",
    "        print_separator()\n",
    "        print(f\"üïì Waktu Prediksi        : {timestamp}\")\n",
    "        print(f\"üóíÔ∏è  Teks Asli            : {text}\")\n",
    "        print(f\"üëâ Label                 : {'Aduan' if label else 'Bukan Aduan'}\")\n",
    "        print(f\"üî¢ Probabilitas          : {prob:.4f}\")\n",
    "        print(f\"üßÆ Panjang Teks          : {char_len} karakter\")\n",
    "        print(f\"üìè Jumlah Token          : {token_len} kata\")\n",
    "        print(f\"‚è±Ô∏è  Durasi Prediksi      : {durasi:.4f} detik\")\n",
    "        print_separator(\"-\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
